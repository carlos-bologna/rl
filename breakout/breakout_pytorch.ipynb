{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
    "\n",
    "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
    "\n",
    "MEMORY_CAPACITY = 50000\n",
    "\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # Number of completely random actions, before the agent starts learning\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "TARGET_UPDATE = 10000           # Number of chosen actions between updating the target network. \n",
    "                                # According to Mnih et al. 2015 this is measured in the number of \n",
    "                                # parameter updates (every four actions), however, in the \n",
    "                                # DeepMind code, it is clearly measured in the number\n",
    "                                # of actions the agent choses\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "IMG_TARGET_SIZE = (84, 84, 1)\n",
    "\n",
    "N_ACTIONS = env.action_space.n # Get number of actions from gym action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replaymemory import ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameBuffer(object):\n",
    "\n",
    "    def __init__(self, shape, n_frames=4):\n",
    "        b, c, h, w = shape\n",
    "        self.capacity = n_frames\n",
    "        self.framebuffer = np.zeros((b, c * n_frames, h, w), 'float32')\n",
    "\n",
    "    def push(self, state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.framebuffer = np.roll(self.framebuffer, -1, axis=1)\n",
    "        self.framebuffer[0, 3] = state\n",
    "        \n",
    "        if self.__len__() < self.capacity:\n",
    "            self.framebuffer = np.repeat(self.framebuffer[:,3,:,:], [self.capacity], axis=0)\n",
    "            self.framebuffer = np.expand_dims(self.framebuffer, axis=0)\n",
    "\n",
    "    def pull(self):\n",
    "\n",
    "        if self.__len__()==0: raise Exception('Framebuffer empty.')\n",
    "\n",
    "        return torch.tensor(self.framebuffer, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.sum(np.sum(self.framebuffer, axis=(2,3)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, outputs):\n",
    "        \n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.relu_gain = nn.init.calculate_gain('relu')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.conv1.weight, gain=self.relu_gain)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.conv2.weight, gain=self.relu_gain)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.conv3.weight, gain=self.relu_gain)\n",
    "        \n",
    "        self.value = nn.Linear(3136, 1)\n",
    "        self.advantage = nn.Linear(3136, outputs)\n",
    " \n",
    "    def forward(self, x):\n",
    "        assert (x.size()[2] == 84) & (x.size()[3] == 84), \"Wrong h ou w size\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        advantage = self.advantage(x.view(x.size(0), -1))\n",
    "        value     = self.value(x.view(x.size(0), -1))\n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "policy_net = DuelingDQN(N_ACTIONS).to(device)\n",
    "target_net = DuelingDQN(N_ACTIONS).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def get_screen():\n",
    "    \n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    \n",
    "    screen_height, screen_width, _ = screen.shape\n",
    "    \n",
    "    screen = screen[int(screen_height*0.28):int(screen_height * 0.92), :]\n",
    "    \n",
    "    view_width = int(screen_width * 0.95)\n",
    "\n",
    "    slice_range = slice(screen_width - view_width, view_width)\n",
    "\n",
    "    # Strip off the edges\n",
    "    screen = screen[:, slice_range, :]\n",
    "    \n",
    "    screen = screen.mean(2, keepdims=True) # 3 channel to 1\n",
    "    \n",
    "    screen = resize(screen, IMG_TARGET_SIZE) / 255.\n",
    "    \n",
    "    screen = screen.transpose((2, 0, 1))\n",
    "    \n",
    "    screen = np.expand_dims(screen, axis=0)\n",
    "\n",
    "    return screen\n",
    "\n",
    "#env.reset()\n",
    "#plt.figure()\n",
    "#plt.imshow(get_screen()[0,0,:,:], cmap='gray')\n",
    "#plt.title('Example extracted screen')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationExploitationScheduler(object):\n",
    "    \"\"\"Determines an action according to an epsilon greedy strategy with annealing epsilon\"\"\"\n",
    "    def __init__(self, net, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01, \n",
    "                 eps_evaluation=0.0, eps_annealing_frames=1000000, \n",
    "                 replay_memory_start_size=50000, max_frames=25000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            DQN: A DQN object\n",
    "            n_actions: Integer, number of possible actions\n",
    "            eps_initial: Float, Exploration probability for the first \n",
    "                replay_memory_start_size frames\n",
    "            eps_final: Float, Exploration probability after \n",
    "                replay_memory_start_size + eps_annealing_frames frames\n",
    "            eps_final_frame: Float, Exploration probability after max_frames frames\n",
    "            eps_evaluation: Float, Exploration probability during evaluation\n",
    "            eps_annealing_frames: Int, Number of frames over which the \n",
    "                exploration probabilty is annealed from eps_initial to eps_final\n",
    "            replay_memory_start_size: Integer, Number of frames during \n",
    "                which the agent only explores\n",
    "            max_frames: Integer, Total number of frames shown to the agent\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "        self.replay_memory_start_size = replay_memory_start_size\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "        self.steps_done = 0\n",
    "        self.net = net\n",
    "\n",
    "    def select_action(self, state, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: A (4, 84, 84) sequence of frames of an Atari game in grayscale\n",
    "            evaluation: A boolean saying whether the agent is being evaluated\n",
    "        Returns:\n",
    "            An integer between 0 and n_actions - 1 determining the action the agent perfoms next\n",
    "        \"\"\"\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if evaluation:\n",
    "            eps = self.eps_evaluation\n",
    "        elif self.steps_done < self.replay_memory_start_size:\n",
    "            eps = self.eps_initial\n",
    "        elif self.steps_done >= self.replay_memory_start_size and self.steps_done < self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope * self.steps_done + self.intercept\n",
    "        elif self.steps_done >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope_2 * self.steps_done + self.intercept_2\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "        return self.net(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards():\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = [15, 20]\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    loss_t = torch.tensor(episode_loss, dtype=torch.float)\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.title('Training...')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.plot(rewards_t.numpy())\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg Loss')\n",
    "    plt.plot(loss_t.numpy())\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Threshold - Epsilon')\n",
    "    plt.plot(episode_threshold)\n",
    "    plt.show()\n",
    "    \n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(rewards_t) >= REPLAY_MEMORY_START_SIZE:\n",
    "        plt.subplot(4, 1, 4)\n",
    "        plt.title('Average Rewards')\n",
    "        plt.xlabel('# Rewards')\n",
    "        plt.ylabel('Avg')\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    #plt.pause(1)  # pause a bit so that plots are updated\n",
    "    \n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    #    display.display(plt.gcf())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "\n",
    "    if len(memory) < REPLAY_MEMORY_START_SIZE:\n",
    "        return\n",
    "    \n",
    "    tree_idx, batch, ISWeights_mb = memory.sample(BATCH_SIZE)\n",
    "      \n",
    "    state_batch = [each[0][0] for each in batch]\n",
    "    action_batch = [each[0][1] for each in batch]\n",
    "    reward_batch = [each[0][2] for each in batch]\n",
    "    next_states_batch = [each[0][3] for each in batch]\n",
    "    dones_batch = [each[0][4] for each in batch]\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          next_states_batch)), device=device, dtype=torch.uint8)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in next_states_batch\n",
    "                                                if s is not None])\n",
    "    \n",
    "    #state_batch = torch.cat([s for s in state_batch if s is not None])\n",
    "    state_batch = torch.cat(state_batch)\n",
    "    action_batch = torch.cat(action_batch)\n",
    "    reward_batch = torch.cat(reward_batch).unsqueeze(1)\n",
    "    #next_states_batch = torch.cat([s for s in next_states_batch if s is not None])\n",
    "    is_not_done = (1 - torch.tensor(dones_batch, dtype=torch.float)).unsqueeze(1).to(device)\n",
    "                \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch) * is_not_done\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    \n",
    "    action_next_state = torch.zeros(BATCH_SIZE, device=device, dtype=torch.long)\n",
    "    next_state_values = torch.zeros((BATCH_SIZE,1), device=device)\n",
    "    \n",
    "    action_next_state[non_final_mask] = policy_net(non_final_next_states).argmax(1)\n",
    "    \n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, action_next_state.unsqueeze(1))\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    # Write loss into Tensorboard\n",
    "    tensorboard.add_scalar('Loss', loss.item(), explore_exploit_sched.steps_done)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    prios = (state_action_values - expected_state_action_values).cpu().detach().numpy().squeeze()\n",
    "    \n",
    "    memory.batch_update(tree_idx, prios)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "screen_batch, screen_channel, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "explore_exploit_sched = ExplorationExploitationScheduler(policy_net, N_ACTIONS, \n",
    "        replay_memory_start_size = REPLAY_MEMORY_START_SIZE, max_frames = MAX_FRAMES)\n",
    "\n",
    "frame_buffer = FrameBuffer(shape=(screen_batch, screen_channel, screen_height, screen_width))\n",
    "\n",
    "rewards_list = np.zeros(10)\n",
    "\n",
    "for _ in range(MAX_FRAMES):\n",
    "    \n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    s = get_screen()\n",
    "\n",
    "    frame_buffer.push(s)\n",
    "    state = frame_buffer.pull()\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(MAX_EPISODE_LENGTH):\n",
    "        \n",
    "        # Select and perform an action\n",
    "        action = explore_exploit_sched.select_action(state)\n",
    "        \n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        # Get next state with frame buffer\n",
    "        next_s = get_screen()\n",
    "        frame_buffer.push(next_s)\n",
    "        next_state = frame_buffer.pull()\n",
    "        \n",
    "        # Store the transition in memory (for Replay)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        loss = optimize_model()\n",
    "\n",
    "        if done:\n",
    "            rewards_list = np.append(rewards_list, total_reward)\n",
    "            rewards_list = np.delete(rewards_list, 0)\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if explore_exploit_sched.steps_done % 10 == 0:\n",
    "            tensorboard.add_scalar('Rewards', rewards_list.mean(), explore_exploit_sched.steps_done)\n",
    "            \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        # Save model\n",
    "        if explore_exploit_sched.steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            torch.save(target_net.state_dict(), 'model/target_net_state_dict.pt')\n",
    "\n",
    "print('Complete')\n",
    "#env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
